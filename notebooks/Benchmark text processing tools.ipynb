{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing tools benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load french text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/cache/epub/5711/pg5711.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request.urlopen(url)\n",
    "rawfr = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Germinal, by Emile Zola\r\n",
      "(#8 in our series by Emile Zola)\r\n",
      "\r\n",
      "Copyright laws are changing all over the world. Be sure to check the\r\n",
      "copyright laws for your country before downloading or redistributing\r\n",
      "this or any other Project Gutenberg eBook.\r\n",
      "\r\n",
      "This header should be the first thing seen when viewing this Project\r\n",
      "Gutenberg file.  Please do not remove it.  Do not change or edit the\r\n",
      "header without written permission.\r\n",
      "\r\n",
      "Please read the \"legal small print,\" and ot\n"
     ]
    }
   ],
   "source": [
    "print(rawfr[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rawfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046377"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawfr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load english text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = request.urlopen(url)\n",
    "raw = response.read().decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\r\n",
      "\r\n",
      "This eBook is for the use of anyone anywhere at no cost and with\r\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\r\n",
      "re-use it under the terms of the Project Gutenberg License included\r\n",
      "with this eBook or online at www.gutenberg.org\r\n",
      "\r\n",
      "\r\n",
      "Title: Crime and Punishment\r\n",
      "\r\n",
      "Author: Fyodor Dostoevsky\r\n",
      "\r\n",
      "Release Date: March 28, 2006 [EBook #2554]\r\n",
      "Last Updated: October 27, 2016\r\n",
      "\r\n",
      "Language: English\r\n",
      "\r\n",
      "Charac\n"
     ]
    }
   ],
   "source": [
    "print(raw[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176967"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Several tokenizers are available. As you will see bellow, spaCy is much faster than the other implementations (Moses, NLTK) and often return better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nautilus_nlp.preprocessing.tokenizer import tokenize, untokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.84 s, sys: 84.1 ms, total: 1.93 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized = tokenize(rawfr[:1000000], lang_module=\"fr_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 ms ± 12.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenized = tokenize(rawfr[:1000000], lang_module=\"fr_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenize(rawfr[:1000000], lang_module=\"fr_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French Moses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 s, sys: 18.9 ms, total: 15.6 s\n",
      "Wall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_moses = tokenize(rawfr[:1000000], lang_module=\"fr_moses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.78 s ± 45.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenized_moses = tokenize(rawfr[:1000000], lang_module=\"fr_moses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_moses = tokenize(rawfr[:1000000], lang_module=\"fr_moses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spacy: 227009 tokens\n",
      "moses: 202475 tokens\n"
     ]
    }
   ],
   "source": [
    "print('spacy: {} tokens\\nmoses: {} tokens'.format(len(tokenized),len(tokenized_moses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['se', 'diriger', 'vers', 'les', '\\r\\n', 'bâtiments', ',', 'il', 'se', 'risqua', 'enfin', 'à', 'gravir', 'le', 'terri', 'sur', 'lequel', 'brûlaient', '\\r\\n', 'les', 'trois', 'feux', 'de', 'houille', ',', 'dans', 'des', 'corbeilles', 'de', 'fonte', ',', 'pour', 'éclairer', '\\r\\n', 'et', 'réchauffer', 'la', 'besogne', '.', ' ', 'Les', 'ouvriers', 'de', 'la', 'coupe', 'à', 'terre', 'avaient', 'dû', '\\r\\n', 'travailler', 'tard', ',', 'on', 'sortait', 'encore', 'les', 'débris', 'inutiles', '.', ' ', 'Maintenant', ',', '\\r\\n', 'il', 'entendait', 'les', 'moulineurs', 'pousser', 'les', 'trains', 'sur', 'les', 'tréteaux', ',', 'il', '\\r\\n', 'distinguait', 'des', 'ombres', 'vivantes', 'culbutant', 'les', 'berlines', ',', 'près', 'de', 'chaque', '\\r\\n', 'feu', '.', '\\r\\n\\r\\n', '--Bonjour', ',', 'dit', '-', 'il', 'en', \"s'\", 'approchant', \"d'\", 'une', 'des', 'corbeilles', '.', '\\r\\n\\r\\n', 'Tournant', 'le', 'dos', 'au', 'brasier', ',', 'le', 'charretier', 'était', 'debout', ',', 'un', 'vieillard', '\\r\\n', 'vêtu', \"d'\", 'un', 'tricot', 'de', 'laine', 'violette', ',', 'coiffé', \"d'\", 'une', 'casquette', 'en', 'poil', 'de', '\\r\\n', 'lapin', ';', 'pendant', 'que', 'son', 'cheval', ',', 'un', 'gros', 'cheval', 'jaune', ',', 'attendait', ',', 'dans', '\\r\\n', 'une', 'immobilité', 'de', 'pierre', ',', \"qu'\", 'on', 'eût', 'vidé', 'les', 'six', 'berlines', 'montées', 'par', '\\r\\n', 'lui', '.', ' ', 'Le', 'manoeuvre', 'employé', 'au', 'culbuteur', ',', 'un', 'gaillard', 'roux', 'et', '\\r\\n', 'efflanqué', ',', 'ne', 'se', 'pressait', 'guère', ',', 'pesait', 'sur', 'le', 'levier', \"d'\", 'une', 'main', '\\r\\n', 'endormie', '.', ' ', 'Et']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized[1000:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['le', 'dos', 'au', 'brasier', ',', 'le', 'charretier', 'était', 'debout', ',', 'un', 'vieillard', 'vêtu', \"d'\", 'un', 'tricot', 'de', 'laine', 'violette', ',', 'coiffé', \"d'\", 'une', 'casquette', 'en', 'poil', 'de', 'lapin', ';', 'pendant', 'que', 'son', 'cheval', ',', 'un', 'gros', 'cheval', 'jaune', ',', 'attendait', ',', 'dans', 'une', 'immobilité', 'de', 'pierre', ',', \"qu'\", 'on', 'eût', 'vidé', 'les', 'six', 'berlines', 'montées', 'par', 'lui', '.', 'Le', 'manoeuvre', 'employé', 'au', 'culbuteur', ',', 'un', 'gaillard', 'roux', 'et', 'efflanqué', ',', 'ne', 'se', 'pressait', 'guère', ',', 'pesait', 'sur', 'le', 'levier', \"d'\", 'une', 'main', 'endormie', '.', 'Et', ',', 'là-haut', ',', 'le', 'vent', 'redoublait', ',', 'une', 'bise', 'glaciale', ',', 'dont', 'les', 'grandes', 'haleines', 'régulières', 'passaient', 'comme', 'des', 'coups', 'de', 'faux', '.', '--Bonjour', ',', 'répondit', 'le', 'vieux', '.', 'Un', 'silence', 'se', 'fit', '.', \"L'\", 'homme', ',', 'qui', 'se', 'sentait', 'regardé', \"d'\", 'un', 'oeil', 'méfiant', ',', 'dit', 'son', 'nom', 'tout', 'de', 'suite', '.', '--Je', 'me', 'nomme', 'Étienne', 'Lantier', ',', 'je', 'suis', 'machineur', '...', 'Il', \"n'\", 'y', 'a', 'pas', 'de', 'travail', 'ici', '?', 'Les', 'flammes', \"l'\", 'éclairaient', ',', 'il', 'devait', 'avoir', 'vingt', 'et', 'un', 'ans', ',', 'très', 'brun', ',', 'joli', 'homme', ',', \"l'\", 'air', 'fort', 'malgré', 'ses', 'membres', 'menus', '.', 'Rassuré', ',', 'le', 'charretier', 'hochait', 'la', 'tête', '.', '--Du', 'travail', 'pour', 'un', 'machineur', ',', 'non', ',']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_moses[1000:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_eng = tokenize(raw[:1000000], lang_module=\"en_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng = tokenize(raw[:1000000], lang_module=\"en_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260 ms ± 1.28 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenize(raw[:1000000], lang_module=\"en_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eng[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.56 s, sys: 28 ms, total: 1.58 s\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_eng_nltk = tokenize(raw[:1000000], lang_module=\"en_nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng_nltk = tokenize(raw[:1000000], lang_module=\"en_nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.54 s ± 27.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "tokenize(raw[:1000000], lang_module=\"en_nltk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_eng_nltk[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus_nlp.preprocessing.stemming import stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 s, sys: 20 ms, total: 4.21 s\n",
      "Wall time: 4.21 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stem = stem_tokens(tokenized,lang='french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25 s ± 61.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "stem_tokens(tokenized,lang='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "from nautilus_nlp.preprocessing.lemmatization import lemmatize_french_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus_nlp.preprocessing.preprocess import remove_tokens_with_nonletters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized = remove_tokens_with_nonletters(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.9 s, sys: 8.62 s, total: 30.6 s\n",
      "Wall time: 30.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatized_tokens = lemmatize_french_tokens(tokenized, module='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_tokens = lemmatize_french_tokens(tokenized, module='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['de', 'puits', 'le', 'vaste', 'chambre', 'de', 'le', 'machine', 'extraction', 'le', 'tourelle', 'de', 'le', 'pompe', 'ce', 'fosse', 'au', 'fondre', 'un', 'creux', 'avec', 'son', 'construction', 'trapu', 'de', 'brique', 'dresser', 'son', 'comme', 'un', 'corne', 'luire', 'sembler', 'avoir', 'un', 'air', 'mauver', 'de', 'goulu', 'accroupie', 'pour', 'manger', 'le', 'monde', 'tout', 'en', 'examiner', 'il', 'songer', 'luire', 'son', 'existence', 'de', 'vagabond', 'depuis', 'huit', 'jour', 'il', 'chercher', 'un', 'place', 'il', 'se', 'revoir', 'dans', 'son', 'atelier', 'de', 'chemin', 'de', 'fer', 'gifler', 'son', 'chef', 'de', 'Lille', 'de', 'partout', 'le', 'samedi', 'il', 'Marchiennes', 'on', 'dire', 'il', 'y', 'avoir', 'de', 'travail', 'aux', 'Forges', 'et', 'rien', 'ni', 'aux', 'Forges', 'ni', 'chez', 'Sonneville', 'il', 'avoir', 'passer', 'le', 'dimanche', 'sou', 'le', 'bois', 'un', 'chantier', 'de', 'charronnage', 'dont', 'le', 'surveillant', 'venir', 'de', 'expulser', 'deux', 'heure', 'de', 'le', 'nuit', 'Rien', 'plus', 'un', 'sou', 'pas', 'un', 'aller', 'il', 'faire', 'ainsi', 'par', 'le', 'chemin', 'sans', 'but', 'ne', 'savoir', 'seulement', 'abriter', 'contre', 'le', 'bise', 'oui', 'bien', 'un', 'fosse', 'le', 'rare', 'lanterne', 'le', 'carreau', 'un', 'porte', 'brusquement', 'ouvrir', 'luire', 'avoir', 'permettre', 'entrevoir', 'le', 'foyer', 'un', 'dans', 'un', 'vive', 'il', 'expliquer', 'de', 'le', 'pompe', 'ce', 'respiration', 'gros', 'et', 'long', 'souffler', 'sans', 'qui', 'comme', 'halein', 'de', 'monstre', 'le', 'manoeuvre', 'de', 'culbuteur', 'gonfler', 'le', 'dos', 'avoir', 'pas', 'le', 'oeil', 'sur', 'et', 'celui', 'ci', 'aller']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_tokens[1000:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nautilus_nlp.preprocessing.lemmatization import lemmatize_english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eng = remove_tokens_with_nonletters(tokenized_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 s, sys: 11.3 s, total: 35.6 s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatized_eng = lemmatize_english_tokens(tokenized_eng, module='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_eng = lemmatize_english_tokens(tokenized_eng, module='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.5 s, sys: 706 ms, total: 21.2 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lemmatized_eng_nltk = lemmatize_english_tokens(tokenized_eng, module='nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_eng_nltk = lemmatize_english_tokens(tokenized_eng, module='nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'ashamed', 'He', 'was', 'hopelessly', 'in', 'debt', 'to', 'his', 'landlady', 'and', 'was', 'afraid', 'of', 'meeting', 'her', 'This', 'was', 'not', 'because', 'he', 'was', 'cowardly', 'and', 'abject', 'quite', 'the', 'contrary', 'but', 'for', 'some', 'time', 'past', 'he', 'had', 'been', 'in', 'an', 'overstrained', 'irritable', 'condition', 'verging', 'on', 'hypochondria', 'He', 'had', 'become', 'so', 'completely', 'absorbed', 'in', 'himself', 'and', 'isolated', 'from', 'his', 'fellows', 'that', 'he', 'dreaded', 'meeting', 'not', 'only', 'his', 'landlady', 'but', 'anyone', 'at', 'all', 'He', 'was', 'crushed', 'by', 'poverty', 'but', 'the', 'anxieties', 'of', 'his', 'position', 'had', 'of', 'late', 'ceased', 'to', 'weigh', 'upon', 'him', 'He', 'had', 'given', 'up', 'attending', 'to', 'matters', 'of', 'practical', 'importance', 'he', 'had', 'lost', 'all', 'desire', 'to', 'do', 'so', 'Nothing', 'that', 'any', 'landlady', 'could', 'do', 'had', 'a', 'real', 'terror', 'for', 'him', 'But', 'to', 'be', 'stopped', 'on', 'the', 'stairs', 'to', 'be', 'forced', 'to', 'listen', 'to', 'her', 'trivial', 'irrelevant', 'gossip', 'to', 'pestering', 'demands', 'for', 'payment', 'threats', 'and', 'complaints', 'and', 'to', 'rack', 'his', 'brains', 'for', 'excuses', 'to', 'prevaricate', 'to', 'lie', 'no', 'rather', 'than', 'that', 'he', 'would', 'creep', 'down', 'the', 'stairs', 'like', 'a', 'cat', 'and', 'slip', 'out', 'unseen', 'This', 'evening', 'however', 'on', 'coming', 'out', 'into', 'the', 'street', 'he', 'became', 'acutely', 'aware', 'of', 'his', 'fears', 'I', 'want', 'to', 'attempt', 'a', 'thing', 'like', 'that', 'and', 'am', 'frightened', 'by', 'these']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_eng[1000:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'ashamed', '-PRON-', 'be', 'hopelessly', 'in', 'debt', 'to', '-PRON-', 'landlady', 'and', 'be', 'afraid', 'of', 'meet', '-PRON-', 'This', 'be', 'not', 'because', '-PRON-', 'be', 'cowardly', 'and', 'abject', 'quite', 'the', 'contrary', 'but', 'for', 'some', 'time', 'past', '-PRON-', 'have', 'be', 'in', 'an', 'overstrained', 'irritable', 'condition', 'verge', 'on', 'hypochondria', '-PRON-', 'have', 'become', 'so', 'completely', 'absorb', 'in', '-PRON-', 'and', 'isolate', 'from', '-PRON-', 'fellow', 'that', '-PRON-', 'dread', 'meeting', 'not', 'only', '-PRON-', 'landlady', 'but', 'anyone', 'at', 'all', '-PRON-', 'be', 'crush', 'by', 'poverty', 'but', 'the', 'anxiety', 'of', '-PRON-', 'position', 'have', 'of', 'late', 'cease', 'to', 'weigh', 'upon', '-PRON-', '-PRON-', 'have', 'give', 'up', 'attend', 'to', 'matter', 'of', 'practical', 'importance', '-PRON-', 'have', 'lose', 'all', 'desire', 'to', 'do', 'so', 'Nothing', 'that', 'any', 'landlady', 'could', 'do', 'have', 'a', 'real', 'terror', 'for', '-PRON-', 'but', 'to', 'be', 'stop', 'on', 'the', 'stair', 'to', 'be', 'force', 'to', 'listen', 'to', '-PRON-', 'trivial', 'irrelevant', 'gossip', 'to', 'pester', 'demand', 'for', 'payment', 'threat', 'and', 'complaint', 'and', 'to', 'rack', '-PRON-', 'brain', 'for', 'excuse', 'to', 'prevaricate', 'to', 'lie', 'no', 'rather', 'than', 'that', '-PRON-', 'would', 'creep', 'down', 'the', 'stair', 'like', 'a', 'cat', 'and', 'slip', 'out', 'unseen', 'This', 'evening', 'however', 'on', 'come', 'out', 'into', 'the', 'street', '-PRON-', 'become', 'acutely', 'aware', 'of', '-PRON-', 'fear', '-PRON-', 'want', 'to', 'attempt', 'a', 'thing', 'like', 'that', 'and', 'be', 'frighten', 'by', 'these']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_eng[1000:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'ashamed', 'He', 'be', 'hopelessly', 'in', 'debt', 'to', 'his', 'landlady', 'and', 'be', 'afraid', 'of', 'meeting', 'her', 'This', 'be', 'not', 'because', 'he', 'be', 'cowardly', 'and', 'abject', 'quite', 'the', 'contrary', 'but', 'for', 'some', 'time', 'past', 'he', 'have', 'be', 'in', 'an', 'overstrain', 'irritable', 'condition', 'verge', 'on', 'hypochondria', 'He', 'have', 'become', 'so', 'completely', 'absorbed', 'in', 'himself', 'and', 'isolated', 'from', 'his', 'fellow', 'that', 'he', 'dread', 'meeting', 'not', 'only', 'his', 'landlady', 'but', 'anyone', 'at', 'all', 'He', 'be', 'crush', 'by', 'poverty', 'but', 'the', 'anxiety', 'of', 'his', 'position', 'have', 'of', 'late', 'cease', 'to', 'weigh', 'upon', 'him', 'He', 'have', 'give', 'up', 'attend', 'to', 'matter', 'of', 'practical', 'importance', 'he', 'have', 'lose', 'all', 'desire', 'to', 'do', 'so', 'Nothing', 'that', 'any', 'landlady', 'could', 'do', 'have', 'a', 'real', 'terror', 'for', 'him', 'But', 'to', 'be', 'stop', 'on', 'the', 'stair', 'to', 'be', 'force', 'to', 'listen', 'to', 'her', 'trivial', 'irrelevant', 'gossip', 'to', 'pester', 'demand', 'for', 'payment', 'threat', 'and', 'complaint', 'and', 'to', 'rack', 'his', 'brain', 'for', 'excuse', 'to', 'prevaricate', 'to', 'lie', 'no', 'rather', 'than', 'that', 'he', 'would', 'creep', 'down', 'the', 'stair', 'like', 'a', 'cat', 'and', 'slip', 'out', 'unseen', 'This', 'even', 'however', 'on', 'come', 'out', 'into', 'the', 'street', 'he', 'become', 'acutely', 'aware', 'of', 'his', 'fear', 'I', 'want', 'to', 'attempt', 'a', 'thing', 'like', 'that', 'and', 'be', 'frighten', 'by', 'these']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_eng_nltk[1000:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
