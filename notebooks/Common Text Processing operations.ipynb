{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing French and English texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hugo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nautilus_nlp.utils.tokenizer import tokenize, untokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fr_txt = \"Ceci est un texte français, j'adore 1 !\"\n",
    "eng_txt = \"Let's play together!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.5 ms, sys: 3.68 ms, total: 27.2 ms\n",
      "Wall time: 30.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Ceci, est, un, texte, français, ,, j', adore, 1, !]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tokenize(fr_txt, lang_module=\"fr_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.5 s, sys: 157 ms, total: 19.6 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_fr_txt = tokenize(fr_txt, lang_module=\"fr_moses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.3 ms, sys: 3.99 ms, total: 28.3 ms\n",
      "Wall time: 26.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_eng_txt = tokenize(eng_txt, lang_module=\"en_spacy\")\n",
    "tokenized_eng_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.3 ms, sys: 3.71 ms, total: 21 ms\n",
      "Wall time: 26.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized_eng_txt = tokenize(eng_txt, lang_module=\"en_nltk\")\n",
    "tokenized_eng_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can also untokenize your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's play together!\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "untokenize(tokenized_eng_txt,lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ceci est un texte français, j' adore !\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here the \"J'adore\" is not handled in the right way\n",
    "untokenize(tokenized_fr_txt,lang='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus_nlp.utils.stemmer import stem_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'surviv', 'these', 'dog']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens(['I','survived','these', 'dogs'], lang='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['je', 'mang', 'dan', 'le', 'cuisin', 'du', 'château']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_tokens(tokenize(\"je mangerai dans les cuisines du château\"),lang='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## French "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nautilus_nlp.utils.lemmatizer import lemmatize_french_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ceci', 'est', 'un', 'texte', 'français', ',', \"j'\", 'adore', 'tes', 'frites', 'bien', 'grasses', 'YOLO', '!']\n"
     ]
    }
   ],
   "source": [
    "txt_to_tokenize=['Ceci', 'est', 'un', 'texte', 'français', ',', \"j'\", 'adore', 'tes', 'frites', 'bien', 'grasses', 'YOLO', '!']\n",
    "print(txt_to_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.46 s, sys: 286 ms, total: 2.74 s\n",
      "Wall time: 2.82 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ceci',\n",
       " 'être',\n",
       " 'un',\n",
       " 'texte',\n",
       " 'français',\n",
       " ',',\n",
       " \"j'\",\n",
       " 'adorer',\n",
       " 'tes',\n",
       " 'frire',\n",
       " 'bien',\n",
       " 'gras',\n",
       " 'YOLO',\n",
       " '!']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Ici frites est traduit par frire car par défaut la fonction remplace le verbe en dernier. \n",
    "# Si ca ne conviens pas au besoin il faut construire sa propre règle de priorisation avec la lib FrenchLefffLemmatizer\n",
    "lemmatize_french_tokens(txt_to_tokenize, module='french_leff_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 131 ms, total: 2.39 s\n",
      "Wall time: 2.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Ceci',\n",
       " 'est',\n",
       " 'un',\n",
       " 'texte',\n",
       " 'français',\n",
       " ',',\n",
       " \"j'\",\n",
       " 'adore',\n",
       " 'tes',\n",
       " 'frite',\n",
       " 'bien',\n",
       " 'grasses',\n",
       " 'YOLO',\n",
       " '!']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Ici on ne remplace que les noms. Frites deviens frite. \n",
    "lemmatize_french_tokens(txt_to_tokenize,load_only_pos='n', module='french_leff_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.9 ms, sys: 51.9 ms, total: 89.8 ms\n",
      "Wall time: 65.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ceci',\n",
       " 'être',\n",
       " 'un',\n",
       " 'texte',\n",
       " 'français',\n",
       " ',',\n",
       " 'j',\n",
       " \"'\",\n",
       " 'adorer',\n",
       " 't',\n",
       " 'frite',\n",
       " 'bien',\n",
       " 'gras',\n",
       " 'yolo',\n",
       " '!']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lemmatize_french_tokens(txt_to_tokenize, module='spacy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hugo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nautilus_nlp.utils.lemmatizer import lemmatize_english_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_lemmatize = ['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.2 ms, sys: 4.4 ms, total: 31.6 ms\n",
      "Wall time: 31.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the', 'strip', 'bat', 'be', 'hang', 'on', '-PRON-', 'foot', 'for', 'good']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lemmatize_english_tokens(to_lemmatize, module='spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 s, sys: 217 ms, total: 3.28 s\n",
      "Wall time: 3.45 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lemmatize_english_tokens(to_lemmatize, module='nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
